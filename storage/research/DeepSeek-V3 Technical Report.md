# DeepSeek-V3 Technical Report

## Resumen

We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly...

## Enlaces

- **Ver en arXiv**: http://arxiv.org/abs/2412.19437v2
- **PDF descargado localmente**: DeepSeek-V3 Technical Report.pdf

---
*Guardado autom√°ticamente desde arXiv*
