# Efficiently Dispatching Flash Attention For Partially Filled Attention Masks

## Resumen

Transformers are widely used across various applications, many of which yield sparse or partially filled attention matrices. Examples include attention masks designed to reduce the quadratic complexity of attention, sequence packing techniques, and recent innovations like tree masking for fast valid...

## Enlaces

- **Ver en arXiv**: http://arxiv.org/abs/2409.15097v2
- **PDF descargado localmente**: Efficiently Dispatching Flash Attention For Partia.pdf

---
*Guardado autom√°ticamente desde arXiv*
