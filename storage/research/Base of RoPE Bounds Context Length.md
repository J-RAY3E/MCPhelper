# Base of RoPE Bounds Context Length

## Resumen

Position embedding is a core component of current Large Language Models (LLMs). Rotary position embedding (RoPE), a technique that encodes the position information with a rotation matrix, has been the de facto choice for position embedding in many LLMs, such as the Llama series. RoPE has been furthe...

## Enlaces

- **Ver en arXiv**: http://arxiv.org/abs/2405.14591v1
- **PDF descargado localmente**: Base of RoPE Bounds Context Length.pdf

---
*Guardado autom√°ticamente desde arXiv*
